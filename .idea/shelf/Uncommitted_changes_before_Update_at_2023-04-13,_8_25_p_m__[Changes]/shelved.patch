Index: fast_hotstuff_bls.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>package fast_hotstuff_bls\n\nimport (\n\t\"bytes\"\n\t\"crypto/sha256\"\n\t\"encoding/binary\"\n\t\"sort\"\n)\n\n// This corresponds to the Algorithm 3 in the Fast-HotStuff\n// paper https://arxiv.org/pdf/2010.11454.pdf\n\n// A BLSPartialSignature is a signature typically made by a validator on a\n// particular payload that can be combined with other signatures from other\n// validators for the same payload. For example, if a hundred validators\n// were to sign Hash(block1.ToBytes()), then all of those signatures could\n// be combined using BLS into a single BLSCombinedSignature.\n//\n// Using BLS signatures is preferred because it's much more space-efficient\n// and computationally efficient than using raw signatures. In addition to\n// condensing n signatures down to a single O(1) value, verifying a\n// combined BLS signature can be done with only one expensive signature\n// check, as opposed to checking n signatures individually.\ntype BLSPartialSignature []byte\n\n// A BLSCombinedSignature is a signature that is the result of combining multiple\n// BLSPartialSignatures on the same payload. In order to verify a BLSCombinedSignature,\n// a validator needs the public keys of all of the validators whose signatures were\n// combined. This is why a secondary field called ValidatorIDBitmap is included\n// in this struct. It allows anyone who receives a BLSCombinedSignature to check it\n// by first looking up the public keys of all of the validators whose signatures were\n// combined, and then verifying the BLSCombinedSignature using those public keys.\n//\n\n// TxnMsg Just creating TxnMsg to avoid errors\ntype TxnMsg struct {\n}\n\ntype Node struct {\n\tId                  int\n\tCurView             uint64\n\tHighestQC           *QuorumCertificate\n\tPubKey              *PublicKey\n\tPrivKey             *PrivateKey\n\tPubKeys             []PublicKey\n\tLatestCommittedView uint64\n}\n\n//\tImportantly, while ValidatorIDBitmap technically requires O(n) space, where n is\n//\n// the number of validators, it can be compressed significantly using a bitmap that\n// only stores the indices of thevalidators whose signatures were combined. For\n// example, because all validators are known at all times, a convention can be used\n// whereby validators are sorted by their public keys, and then the ValidatorIDBitmap\n// is simply a bitmap that stores the indices of the validators whose signatures were\n// combined.\n//\n// This means that even if you have 10,000 validators involved in a signature, you will\n// only need about a kilobyte of space to store the ValidatorIDBitmap, which is only\n// a little more than the size of a typical transaction on the network.\ntype BLSCombinedSignature struct {\n\tCombinedSignature []byte\n\tValidatorIDBitmap []byte\n}\n\n// For the purposes of this code, we will assume that PublicKey and PrivateKey can\n// be used to sign and verify BLS signatures. Note that BLS signatures require a little\n// more setup in order to be use-able, but that setup is not needed for the purposes of\n// this pseudocode.\ntype PublicKey []byte\ntype PrivateKey []byte\n\n// When a validator receives a block that they believe to be valid, they send\n// a VoteMessage back to the leader with a signature that the leader can then\n// package into a QuorumCertificate for that block.\ntype VoteMessage struct {\n\t// The public key of the validator who's sending this message.\n\tValidatorPublicKey PublicKey\n\n\t// The view of the block that this validator is voting for.\n\t// Note that the view isn't explicitly needed here, as the block will contain\n\t// the view regardless, but we include it for convenience and ease of\n\t// debugging.\n\tView uint64\n\n\t// The hash of the block that this validator is voting for.\n\tBlockHash [32]byte\n\n\t// This is a signature of the (View, BlockHash) pair referenced above. It indicates\n\t// that this validator believes this block to be valid at this particular view.\n\tPartialViewBlockHashSignature BLSPartialSignature\n}\n\n// A QuorumCertificate is a collection of signatures from 2/3rds of the validators\n// on the network, weighted by Stake. The signatures are associated with a particular\n// block and a particular view in which that block was proposed.\ntype QuorumCertificate struct {\n\n\t// The hash of the block that this QuorumCertificate authorizes.\n\tBlockHash [32]byte\n\n\t// The view corresponding to the block that this QuorumCertificate authorizes.\n\tView uint64\n\n\t// This signature is a BLSCombinedSignature that is the result of combining\n\t// all of the PartialViewBlockHashSignatures from the VoteMessages that were\n\t// aggregated by a leader. Note that the signature includes the ValidatorIDBitmap\n\t// which the recipient can use to verify the signature and to verify the amount\n\t// of stake that the signers collectively own.\n\tCombinedViewBlockHashSignature BLSCombinedSignature\n}\n\n// Set of SafeBlocks and CommittedBlocks\ntype SafeBlockMap map[[32]byte]*Block\ntype CommittedBlockMap map[[32]byte]*Block\n\n// The TimeoutMessage is sent from a validator to the next leader when that\n// validator wants to timeout on a particular view. It contains the highest QC\n// that the validator is aware of. This QC then allows the leader to link back to\n// the most recent block that 2/3rds of validators are aware of when constructing\n// the next block.\n//\n// When a leader receives a TimeoutMessage for a particular view from 2/3rds of the\n// validators, weighted by Stake, the leader can construct an AggregateQC, which they\n// can then include in a proposed block.\ntype TimeoutMessage struct {\n\n\t// The public key of the validator who's sending this message.\n\tValidatorPublicKey PublicKey\n\n\t// The view that the validator wants to skip over (because they haven't received a\n\t// valid block for it and they timed out).\n\tTimeoutView uint64\n\n\t// The QuorumCertificate with the highest view that the validator is aware\n\t// of. This QC allows the leader to link back to the most recent block that\n\t// 2/3rds of validators are aware of when constructing the next block.\n\tHighQC QuorumCertificate\n\n\t// A signature of (TimeoutView, HighQC.View) that indicates this validator\n\t// wants to timeout. Notice that we include the HighQC.View in the signature\n\t// payload rather than signing the full serialized HighQC itself. This allows the leader\n\t// to better aggregate validator signatures without compromising the integrity\n\t// of the protocol.\n\tPartialTimeoutViewSignature BLSPartialSignature\n}\n\n// AggregateQC is an aggregation of timeout messages from 2/3rds of all validators,\n// weighted by stake, that indicates that these validators want to time out a\n// particular view.\n//\n// The AggregateQC contains the highest QC that all of the validators who timed out\n// are aware of, and combined signatures from all of the validators who timed out.\n//\n// As one piece of additional complexity, note that the signatures of the validators\n// cannot be naively aggregated into a single BLSCombinedSignature, as the signatures\n// are on different payloads, i.e. different (TimeoutView, HighQC.View) pairs. Instead,\n// the leader must group the signatures for each unique payload, and combine them\n// together into a BLSCombinedSignature for each payload.\n//\n// Importantly, the number of signatures should generally be proportional to the\n// number of views that have timed out. And so the AggregateQC should not generally\n// be much larger than a normal QC.\ntype AggregateQC struct {\n\n\t// The view that this AggregateQC corresponds to. This is the view that the\n\t// validators decided to timeout on.\n\tView uint64\n\n\t// The highest QC exctracted from all of the TimeoutMessages that the leader\n\t// received.\n\tValidatorTimeoutHighQC QuorumCertificate\n\n\t// Here we include a list of the HighQC.View values we got from each of the\n\t// validators in the ValidatorTimeoutHighQCViews field. In addition, for each\n\t// unique HighQC.View value we received, we combine all the PartialTimeoutViewSignatures\n\t// for that HighQC.View into a single BLSCombinedSignature. The\n\t// ValidatorCombinedTimeoutSignatures array contains the combined signatures for\n\t// each of the unique views in ValidatorTimeoutHighQCViews.\n\t//todo: Discuss that bls can aggregate msgs with different payloads.\n\t//https://crypto.stanford.edu/~dabo/pubs/papers/BLSmultisig.html\n\n\t// Notice that we can't include a single BLSCombinedSignature for the entire\n\t// set of TimeoutMessages we received because we can only aggregate BLSPartialSignatures\n\t// that have the same payload, i.e. the same (TimeoutView, HighQC.View) pair.\n\t// This should generally be fine, however, because the number of unique HighQC.View\n\t// values should not generally exceed the number of timed-out views.\n\tValidatorTimeoutHighQCViews        []uint64\n\tValidatorCombinedTimeoutSignatures []BLSCombinedSignature\n}\n\n//  Blocks are bundles of transactions proposed by the current leader.\n//\n// If the block from the previous view was a valid block that 2/3rds of validators\n// have seen and validated, then the next block proposed by the leader will link\n// back to that block by including a QuorumCertificate (QC), which bundles signatures from\n// VoteMessages from 2/3rds of validators, weighted by stake, indicating that\n// these validators believe the previous block to be valid. This is the simple case\n// where everything is running normally, with no timeouts, and the AggregateQC field\n// will be left empty in this case.\n//\n// In the event that 2/3rds of validators timed out in the previous view, then the\n// AggregateQC field will be constructed from TimeouteMessages received from 2/3rds\n// of the validators. In this case, the QC will be set to the QC with the highest\n// view number contained in the AggregateQC, since that is the most recent valid\n// block that 2/3rds of validators have seen.\n//\n// The idea is that each block must link to the most recent valid block that 2/3rds\n// of validators have seen. In normal conditions, the next leader will be able to\n// assemble a QC directly from the VoteMessages they receive. In a timeout scenario,\n// they will instead need to aggregate TimeoutMessages, and assemble them\n// into an AggregateQC.\n\ntype Block struct {\n\n\t// The hash of the previous block that this block extends.\n\tPreviousBlockHash [32]byte\n\n\t// The public key of the leader who is proposing this block.\n\tProposerPublicKey PublicKey\n\n\t// List of transactions contained in this block. This data is what all\n\t// the validators are attempting to agree on in the consensus.\n\tTxns []TxnMsg\n\n\t// View in which this block is proposed.\n\tView uint64\n\n\t// QC contains a list of VoteViewBlockHashSignatures from 2/3rds of the validators\n\t// weighted by stake.\n\tQC QuorumCertificate\n\n\t// AggregateQC is set to nil whenever a regular block vote takes place.\n\t// In the case of timeouts, the AggregateQC is set. It serves as proof\n\t// that 2/3rds of the validators, weighted by stake, timed out.\n\tAggregateQC AggregateQC\n\n\t// The signature of the block made by the leader.\n\tProposerSignature BLSPartialSignature\n}\n\n// Some  utility functions\nfunc (node Node) GetMyKeys(mypubkey PublicKey, myprivkey PrivateKey) {\n\tnode.PrivKey = &myprivkey\n\tnode.PubKey = &mypubkey\n}\n\nfunc (Block) Hash() (hash [32]byte) {\n\treturn hash\n}\n\nfunc VerifySignature(hash [32]byte, publicKey PublicKey, signature []byte) bool {\n\treturn true\n}\n\n// ResetTimeout() resets timer for a specific duration. Duration is the funciton of the number of times\n// the protocol observed failure. But it should be capped and grandually decreased to a normal acceptable\n// value.\nfunc (node Node) ResetTimeout() {\n\n}\n\n// Compute the leader node for the given view number and list of public keys\nfunc computeLeader(viewNum uint64, pubKeys []PublicKey) PublicKey {\n\t// Compute the hash of the view number as a byte slice\n\tviewHash := sha256.Sum256([]byte{byte(viewNum), byte(viewNum >> 8), byte(viewNum >> 16), byte(viewNum >> 24),\n\t\tbyte(viewNum >> 32), byte(viewNum >> 40), byte(viewNum >> 48), byte(viewNum >> 56)})\n\n\t// Sort the public keys lexicographically\n\tsort.Slice(pubKeys, func(i, j int) bool {\n\t\treturn bytes.Compare(pubKeys[i], pubKeys[j]) < 0\n\t})\n\n\t// Compute the index of the leader node as the least significant 8 bits of the hash\n\tidx := int(viewHash[31]) % len(pubKeys)\n\n\t// Return the public key of the leader node\n\treturn pubKeys[idx]\n}\n\nfunc Hash(x uint64, y interface{}) [32]byte {\n\tvar buf []byte\n\tswitch y := y.(type) {\n\tcase uint64:\n\t\tbuf = make([]byte, 8)\n\t\tbinary.LittleEndian.PutUint64(buf, y)\n\tcase []byte:\n\t\tbuf = y\n\tdefault:\n\t\tpanic(\"invalid type\")\n\t}\n\th := sha256.Sum256(append(buf, byte(x)))\n\treturn h\n}\n\nfunc (pk PublicKey) Equals(other PublicKey) bool {\n\treturn bytes.Equal(pk, other)\n}\n\n// TimoutDuration This function returns the duration. It should be noted that duration can be calculated as\n// the function of number of failures.\n\n// === Some local variables ===\n\n// The current validator's public and private keys, which should have been registered\n// previously, and which should have stake allocated to them as well.\n\n// The timeout value is initially set to thirty seconds, and then doubled\n// for each consecutive timeout the node experiences, which allows for other nodes\n// to catch up in the event of a network disruption.\n\n// I think this is too much detail for spec. You can decide on how to do it during implementation.\n//var timeoutSeconds = GetInitialTimeout()\n\n// The highest QC that this node has seen so far. This is tracked so that the\n// node can know when a block can be finalized, and so that the highest QC can\n// be sent to the leader in the event of a timeout.\n\n// The networkChannel variable is a wrapper around peer connections. We will use\n// it as an abstraction in this pseudocode to send and receive messages from\n// other peers. For simplicity, we skip the implementation details of the p2p\n// connections.\n\n//networkChannel = ConnectToPeers()\n\n// The currentView variable stores the view that this node is currently on. We use the\n// GetCurrentView() value in this pseudode instead of starting with 0 because\n// we assume the network is in steady-state, rather than starting from the\n// initial conditions, and we leave out all the details of getting in sync with\n// other peers here as well.\n\n// The votesSeen variable stores a map of vote messages seen by the leader in the\n// current view. We will make sure this map only stores votes for the currentView.\ntype votesSeen map[string]*VoteMessage\n\n// The timeoutsSeen variable is similar to votesSeen. It stores the timeout messages\n// seen by the leader in the current view. We also make sure this map only stores\n// timeouts for the current view.\n\n// A node might\ntype TimeoutsSeenMap map[uint64]map[[32]byte][]TimeoutMessage\n\nfunc (m TimeoutsSeenMap) Reset(key uint64) {\n\tdelete(m, key)\n}\n\n// Validates supermajority has voted in QC\nfunc ValidateSuperMajority_QC(BLSCombinedSignature) bool {\n\treturn true\n}\n\n// ValidateSuperMajority_AggQC Validate super majority has sent their timeout msgs\nfunc ValidateSuperMajority_AggQC([]BLSCombinedSignature) bool {\n\t//\n\treturn true\n}\nfunc verifySignaturesAndTxns(block Block) bool {\n\treturn true\n}\nfunc Sign(payload [32]byte, privKey PrivateKey) ([]byte, error) {\n\t//to be implemented\n\treturn []byte(uint64(3)), nil\n}\n\n// The ResetTimeoutAndAdvanceView is used to reset the timeout duration, and the\n// leader maps, as well as to increment the currentView. It’s called once a valid\n// block has been produced for the view.\n\n// The AdvanceView function is used to reset the leader’s vote and timeout maps and\n// increment the currentView.\n\n//votesSeen.Reset() and\n//\tTimeoutsSeenMap.Reset(certificate.View) can be called later whenever needed.\n\nfunc (node Node) AdvanceView_qc(certificate QuorumCertificate) {\n\tcertificate.View += 1\n\tnode.ResetTimeout()\n}\n\nfunc (node Node) AdvanceView_Aggqc(agqc AggregateQC) {\n\tagqc.View += 1\n\tnode.ResetTimeout()\n}\n\n// This functions is used to get index  of the signer of QC in the bitmap.\nfunc getOnBitIndices(bitmap []byte) []int {\n\tindices := make([]int, 0)\n\tfor i := 0; i < len(bitmap)*8; i++ {\n\t\tif getBitAtIndex(bitmap, i) {\n\t\t\tindices = append(indices, i)\n\t\t}\n\t}\n\treturn indices\n}\n\nfunc getBitAtIndex(bitmap []byte, i int) bool {\n\tbyteIndex := i / 8\n\tbitIndex := uint(i % 8)\n\treturn bitmap[byteIndex]&(1<<bitIndex) != 0\n}\n\nfunc Send(msg VoteMessage, leader PublicKey) {\n\n}\n\nfunc containsBlock(blockID [32]byte, committedBlocks *CommittedBlockMap) bool {\n\tfor _, block := range *committedBlocks {\n\t\tif bytes.Equal(block.ID(), blockID[:]) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc GetBlockIDForView(view uint64, blockMap SafeBlockMap) [32]byte {\n\tfor _, block := range blockMap {\n\t\tif block.View == view {\n\t\t\treturn block.Hash()\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (node *Node) tryCommitGrandParent(block *Block, safeBlocks *SafeBlockMap, committedBlocks *CommittedBlockMap) {\n\tparent := (*safeBlocks)[block.QC.BlockHash]\n\tgrandParent := (*safeBlocks)[parent.QC.BlockHash]\n\n\t// this case should just trigger on genesis_case,\n\t// as the preconditions on outer calls should check on block validity\n\tif parent == nil || grandParent == nil {\n\t\treturn\n\t}\n\n\tcanCommit :=\n\t\tparent.View == (grandParent.View + 1)\n\tif canCommit {\n\t\tfor view := node.LatestCommittedView + 1; view <= grandParent.View; view++ {\n\t\t\tblockHash := GetBlockIDForView(grandParent.View, *safeBlocks)\n\t\t\tblock, ok := (*safeBlocks)[blockHash]\n\t\t\tif !ok {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif _, ok := (*committedBlocks)[block.Hash()]; ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t(*committedBlocks)[block.Hash()] = block\n\t\t\tincrementLatestCommittedView(view)\n\t\t}\n\t}\n\n}\n\n// sanityCheckBlock is used to verify that the block contains valid information.\nfunc sanityCheckBlock(block Block, node *Node) bool {\n\t// Make sure the block is for the current view.\n\t//\n\t// Notice that with this code it is technically possible to receive a valid block\n\t// that has a view *less* than the current one. This can happen in a case where\n\t// you timeout, advance the view, and then it turns out that the block you\n\t// timed out on was able to form a QC. This is a general class of issues that is resolved\n\t// with block caching logic that is not included in this pseudocode. Generally,\n\t// you can assume that these situations where you receive a block from a previous\n\t// view are handled by the caching logic. The caching logic would need to essentially\n\t// do all of the same checks as we do here, but skip the checks that are specific to the\n\t// current view.\n\t//\n\t// In addition, it is possible to receive a block that is *ahead* of your current\n\t// view. This can happen in a case where the leader for the current view was able\n\t// to form a QC without your vote, before you were able to receive the block. Again\n\t// this is a general class of issues that is resolved with block caching logic that\n\t// is not included here. This case is simpler than the previous case because you\n\t// would simply not process the future block until downloading and processing the\n\t// previous block referred to by its QC.\n\tif block.View < node.CurView {\n\t\treturn false\n\t}\n\n\t// Check that the block's proposer is the expected leader for the current view.\n\tif !block.ProposerPublicKey.Equals(computeLeader(block.View, node.PubKeys)) {\n\t\treturn false\n\t}\n\n\t// Make sure the leader signed the block.\n\tif !VerifySignature(block.Hash(), block.ProposerPublicKey, block.ProposerSignature) {\n\t\treturn false\n\t}\n\n\t// The block's QC should never be empty.\n\tif &block.QC == nil {\n\t\treturn false\n\t}\n\n\t// We make sure the QC contains valid signatures from 2/3rds of validators, weighted by stake.\n\tif !ValidateSuperMajority_QC(block.QC.CombinedViewBlockHashSignature) {\n\t\treturn false\n\t}\n\n\t// verifySignaturesAndTxns checks the transactions contained in the block are valid and have\n\t// correct signatures. We make sure that the block connects to the rest of the chain.\n\tif !verifySignaturesAndTxns(block) {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\nfunc validateQuorumCertificate(qc QuorumCertificate) bool {\n\t// Make sure that the validators included in the QC collectively own at least 2/3rds\n\t// of the stake. Also make sure there are no repeated public keys.\n\t// Note that we use the Bitmap contained in the combined signature to determine\n\t// which validators' signatures were used, and what their total stake was.\n\tif !ValidateSuperMajority_QC(qc.CombinedViewBlockHashSignature) {\n\t\treturn false\n\t}\n\n\t// Make sure that the BlockHash in the qc matches our local BlockHash history.\n\t//Rev: It is not possible to have a valid quorum certificate and different blockchain history\n\t// Hence, this check is not necessary.\n\t//if GetBlockHashForView(qc.View) != qc.BlockHash {\n\t//\treturn false\n\t//\t}\n\n\treturn true\n}\n\n// todo: This function needs to be revised\nfunc validateTimeoutProof(aggregateQC AggregateQC, pubkeys []PublicKey) bool {\n\t// Make sure the lists in the AggregateQC have equal lengths\n\tif len(aggregateQC.ValidatorTimeoutHighQCViews) != len(aggregateQC.ValidatorCombinedTimeoutSignatures) {\n\t\treturn false\n\t}\n\n\t// Make sure that the validators included in the QC collectively own at least 2/3rds\n\t// of the stake. Also make sure there are no repeated public keys.\n\t// Note the bitmap in the signature allows us to determine how much stake the\n\t// validators had.\n\tif !ValidateSuperMajority_AggQC(aggregateQC.ValidatorCombinedTimeoutSignatures) {\n\t\treturn false\n\t}\n\n\t// Iterate over all the aggregate qc signatures and verify that the signatures are correct.\n\n\t// Rev: Don't need to iterate over all the signatures. Just verify the highQC and the\n\t// aggregated signature of the aggregatedQC.\n\thighestQCView := uint64(0)\n\n\t//for ii := 0; ii < len(aggregateQC.ValidatorTimeoutHighQCViews); ii++ {\n\t//\tpayload := Hash(aggregateQC.View, aggregateQC.ValidatorTimeoutHighQCViews[ii])\n\t//\tif !VerifySignature(payload, ,aggregateQC.ValidatorCombinedTimeoutSignatures[ii]) {\n\t//\t\treturn false\n\t//\t}\n\t//\tif aggregateQC.ValidatorTimeoutHighQCViews[ii] > highestQCView {\n\t//\t\thighestQCView = aggregateQC.ValidatorTimeoutHighQCViews[ii]\n\t//\t}\n\t//\t}\n\n\t// The highest QC view found in the signatures should match the highest view\n\t// of the HighestQC included in the AggregateQC.\n\tif highestQCView != aggregateQC.ValidatorTimeoutHighQC.View {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\n// The handleBlockFromPeer is called whenever we receive a block from a peer.\nfunc handleBlockFromPeer(block *Block, node *Node) {\n\t// Make sure that the block contains a valid QC, signature, transactions,\n\t// and that it's for the current view.\n\tif !sanityCheckBlock(*block, node) {\n\t\treturn\n\t}\n\n\t// The safeVote variable will tell us if we can vote on this block.\n\tsafeVote := false\n\n\t// If the block doesn’t contain an AggregateQC, then that indicates that we\n\t// did NOT timeout in the previous view, which means we should just check that\n\t// the QC corresponds to the previous view.\n\tif &block.AggregateQC == nil {\n\t\t// The block is safe to vote on if it is a direct child of the previous\n\t\t// block. This means that the parent and child blocks have consecutive\n\t\t// views. We use the current block’s QC to find the view of the parent.\n\t\tsafeVote = block.View == block.QC.View+1\n\t\tnode.AdvanceView_qc(block.QC)\n\t} else {\n\t\t// If we have an AggregateQC set on the block, it means the nodes decided\n\t\t// to skip a view by sending TimeoutMessages to the leader, so we process\n\t\t// the block accordingly.\n\n\t\t// First we make sure the block contains a valid AggregateQC.\n\t\tvalidateTimeoutProof(block.AggregateQC, node.PubKeys)\n\t\t// We find the QC with the highest view among the QCs contained in the\n\t\t// AggregateQC.\n\t\thighestTimeoutQC := block.AggregateQC.ValidatorTimeoutHighQC\n\t\t// If our local highestQC has a smaller view than the highestTimeoutQC,\n\t\t// we update our local highestQC.\n\t\tif highestTimeoutQC.View > node.HighestQC.View {\n\t\t\tnode.HighestQC = &(highestTimeoutQC)\n\t\t}\n\t\t// We make sure that the block’s QC matches the view of the highest QC that we’re aware of.\n\t\tsafeVote = block.QC.View == node.HighestQC.View\n\t\tnode.AdvanceView_Aggqc(block.AggregateQC)\n\t}\n\n\t// If safeVote is true, we will vote on the block.\n\tif safeVote {\n\t\t// Construct the vote message. The vote will contain the validator's\n\t\t// signature on the <view, blockHash> pair.\n\t\tpayload := Hash(block.View, block.Hash())\n\t\tblockHashSignature, _ := Sign(payload, *node.PrivKey)\n\n\t\tvoteMsg := VoteMessage{\n\t\t\tValidatorPublicKey:            *node.PubKey,\n\t\t\tView:                          block.View,\n\t\t\tBlockHash:                     block.Hash(),\n\t\t\tPartialViewBlockHashSignature: blockHashSignature,\n\t\t}\n\t\t// Send the vote directly to the next leader.\n\t\tSend(voteMsg, computeLeader(node.CurView+1, node.PubKeys))\n\t\t// We can now proceed to the next view.\n\t\tnode.AdvanceView_qc(block.QC)\n\t}\n\n\t// Our commit rule relies on the fact that blocks were produced without timeouts.\n\t// Check if the chain looks like this:\n\t// B1 - B2 - ... - B_current (current block)\n\t// Where ... represent an arbitrary number of skipped views.\n\tB_current := block\n\tB2 := GetBlockForHash(block.QC.BlockHash)\n\tB1 := GetBlockForHash(B2.QC.BlockHash)\n\n\t// We update the highestQC if the block we received has one with a higher view.\n\t// In the case where we have an AggregateQC, remember that B_current.QC will\n\t// line up with the highest QC contained within the AggregateQC.\n\tif B_current.QC.View > highestQC.View {\n\t\thighestQC = B_current.QC\n\t}\n\n\t// Check that B2 is a direct child of B1.\n\tif B2.View == B1.View+1 {\n\t\t// A direct chain is formed between B1 and B2, reinforced by the QC\n\t\t// in B_current. This means we should commit all blocks up to and including\n\t\t// block B1.\n\t\tCommitUpToBlock(B1)\n\t}\n}\n\nfunc validateVote(vote VoteMessage) bool {\n\t// Make sure the vote is made on the block in the previous view.\n\tif vote.View != currentView-1 {\n\t\treturn false\n\t}\n\n\t// Make sure that the validator is registered.\n\tif !verifyValidatorPublicKey(vote.ValidatorPublicKey) {\n\t\treturn false\n\t}\n\n\t// Make sure that the BlockHash in the view matches our local BlockHash history.\n\tif GetBlockHashForView(vote.View) != vote.BlockHash {\n\t\treturn false\n\t}\n\n\t// Now verify the <view, BlockHash> signature\n\tpayload := Hash(vote.View, vote.BlockHash)\n\tif !VerifySignature(payload, vote.ValidatorPublicKey, vote.PartialViewBlockHashSignature) {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\n// The handleVoteMessageFromPeer is called whenever we receive a vote from a peer.\nfunc handleVoteMessageFromPeer(vote *VoteMessage) {\n\t// If we're not the leader, ignore all votes.\n\tif !IsLeader(currentView) {\n\t\treturn\n\t}\n\n\t// Make sure that the vote is for the currentView and validate\n\t// the vote’s signature. We also run a check to make sure we didn’t\n\t// already receive a timeout or another vote from this peer.\n\tif !validateVote(vote) {\n\t\treturn\n\t}\n\n\t// If we get here, it means we are the leader so add the vote to our map of\n\t// votes seen.\n\tvotesSeen[vote.ValidatorPublicKey] = vote\n\n\t// Check if we’ve gathered votes from 2/3rds of the validators, weighted by stake.\n\tif ComputeVoteStake(votesSeen) < 2/3*GetTotalStake() {\n\t\treturn\n\t}\n\n\t// At this point, we have collected enough votes to know that we can\n\t// propose a block during this view.\n\n\t// Construct the QC and note that the BlockHash references the\n\t// previous block that we're about to build on top of.\n\tqc := QuorumCertificate{\n\t\tView:                           vote.View,\n\t\tBlockHash:                      vote.BlockHash,\n\t\tCombinedViewBlockHashSignature: ConstructCombinedSignatureFromVotes(votesSeen),\n\t}\n\n\t// Construct the block\n\tblock := Block{\n\t\tPreviousBlockHash: qc.BlockHash,\n\t\tProposerPublicKey: myPublicKey,\n\t\tTxns:              GetTxns(),\n\t\tView:              currentView,\n\t\tQC:                qc,\n\t\tAggregateQC:       nil,\n\t}\n\n\t// Sign the block using the leader’s private key.\n\tblock.ProposerSignature = Sign(block.Hash(), myPrivateKey)\n\t// Blast the block to everyone including yourself. This means we'll process\n\t// this block in handleBlockFromPeer, where we'll also update our highestQC and\n\t// advance to the next view. As such, there is no reason to\n\t// call ResetTimeoutAndAdvanceView() here.\n\tbroadcast(block)\n}\n\nfunc validateTimeout(timeout TimeoutMessage) bool {\n\t// Make sure that the validator is registered.\n\tif !verifyValidatorPublicKey(vote.ValidatorPublicKey) {\n\t\treturn false\n\t}\n\n\t// Verify the HighQC in the timeout message\n\tif !validateQuorumCertificate(timeout.HighQC) {\n\t\treturn false\n\t}\n\n\t// Verify the validator signature\n\tpayload := Hash(view, timeout.HighQC.View)\n\tif !VerifySignature(payload, timeout.ValidatorPublicKey, timeout.PartialTimeoutViewSignature) {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\n// The handleTimeoutMessageFromPeer is called whenever we receive a timeout\n// from a peer.\nfunc handleTimeoutMessageFromPeer(timeoutMsg TimeoutMessage) {\n\t// If we're not the leader, ignore all timeout messages.\n\tif !IsLeader(vote.View) {\n\t\treturn\n\t}\n\n\t// Make sure that the timeoutMsg is for the most recent view and validate all of\n\t// its signatures, including those for its HighQC. We also run a check to make\n\t// sure we didn’t already receive a timeout or another vote from this peer.\n\tif !validateTimeout(timeoutMsg) {\n\t\treturn\n\t}\n\n\t// If we get here, it means we are the leader so add the timeoutMsg to our\n\t// map of timeouts seen.\n\ttimeoutsSeenMap[timeoutMsg.ValidatorPublicKey] = timeoutMsg\n\n\t// Check if we’ve gathered timeouts from 2/3rds of the validators, weighted\n\t// by stake.\n\tif ComputeTimeoutStake(timeoutsSeenMap) < 2/3*GetTotalStake() {\n\t\treturn\n\t}\n\n\t// If we get here, it means we have enough timeouts to know that we can\n\t// propose a block during this view.\n\n\t// In order to construct the block, we will need to construct an AggregateQC\n\t// to prove that everyone timed out according to expectations. We compute a\n\t// validatorHighQCs list of all the QCs sent to use by the validators, along\n\t// with their signatures. We also find the QC with the highest view among the\n\t// validatorHighQCs.\n\thighQC, timeoutHighQCViews, timeoutHighQCCombinedSigs := FormatTimeoutQCs(timeoutsSeenMap)\n\t// Construct the AggregateQC for this view.\n\taggregateQC := AggregateQC{\n\t\tView:                               timeoutMsg.TimeoutView,\n\t\tValidatorTimeoutHighQC:             highQC,\n\t\tValidatorTimeoutHighQCViews:        timeoutHighQCViews,\n\t\tValidatorCombinedTimeoutSignatures: timeoutHighQCCombinedSigs,\n\t}\n\n\t// Construct the block and include the aggregateQC.\n\tblock := Block{\n\t\tPreviousBlockHash: highQC.BlockHash,\n\t\tProposerPublicKey: myPublicKey,\n\t\tTxns:              GetTxns(),\n\t\tView:              currentView,\n\t\t// Setting the QC is technically redundant when we have an AggregateQC but\n\t\t// we set it anyway for convenience.\n\t\tQC:          highQC,\n\t\tAggregateQC: aggregateQC,\n\t}\n\n\t// Sign the block using the leader’s private key.\n\tblock.ProposerSignature = Sign(block, myPrivateKey)\n\t// Blast the block to everyone including yourself. This means we'll process\n\t// this block in handleBlockFromPeer, where we'll also update our highestQC and\n\t// advance to the next view.\n\tbroadcast(block)\n}\n\n// This is the node's main message and event handling loop. We continuously loop,\n// incrementing the view with each round.\nfunc StartConsensus() {\n\t// We run an infinite loop, and process each message from our peers as it\n\t// comes in. Note that the timeout is also something we might process as\n\t// part of this main loop. If you are unfamiliar with the concept of a \"select\"\n\t// statement, we recommend referencing Go's implementation here:\n\t// - https://golangdocs.com/select-statement-in-golang\n\tfor {\n\t\tselect {\n\t\tcase messageFromPeer := <-networkChannel.WaitForMessage():\n\t\t\tif messageFromPeer.MessageType() == BlockMessageType {\n\t\t\t\thandleBlockFromPeer(messageFromPeer)\n\n\t\t\t} else if messageFromPeer.MessageType() == VoteMessageType {\n\t\t\t\thandleVoteMessageFromPeer(messageFromPeer)\n\n\t\t\t} else if messageFromPeer.MessageType() == TimeoutMessageType {\n\t\t\t\thandleTimeoutMessageFromPeer(messageFromPeer)\n\n\t\t\t}\n\t\tcase <-WaitForTimeout():\n\t\t\t// WaitForTimeout() is a function that will emit a value\n\t\t\t// whenever a timeout is triggered, causing us to enter this part\n\t\t\t// of the code. It can be assumed that calling\n\t\t\t// ResetTimeout(timeoutValue) will cause WaitForTimeout() to emit\n\t\t\t// a value after timeoutValue seconds have elapsed (ignoring all\n\t\t\t// previous calls to ResetTimeout()).\n\n\t\t\t// Construct the timeout message\n\t\t\ttimeoutMsg := TimeoutMessage{\n\t\t\t\tValidatorPublicKey:          myPublicKey,\n\t\t\t\tTimeoutView:                 currentView,\n\t\t\t\tHighQC:                      highestQC,\n\t\t\t\tPartialTimeoutViewSignature: Sign(Hash(currentView, highestQC.View), myPrivateKey),\n\t\t\t}\n\n\t\t\t// Send the timeout message and send it to the next leader .\n\t\t\tSend(timeoutMsg, computeLeader(currentView+1))\n\n\t\t\t// We use exponential backoff for timeouts in this reference\n\t\t\t// implementation.\n\t\t\tResetTimeoutAndAdvanceView(2 * timeoutSeconds)\n\t\tcase <-WaitForQuitSignal():\n\t\t\tExit()\n\t\t}\n\t}\n}\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/fast_hotstuff_bls.go b/fast_hotstuff_bls.go
--- a/fast_hotstuff_bls.go	(revision 26d917f5f12843e06845f34730a80c4f7cb831bb)
+++ b/fast_hotstuff_bls.go	(date 1681442475993)
@@ -4,6 +4,7 @@
 	"bytes"
 	"crypto/sha256"
 	"encoding/binary"
+	"fmt"
 	"sort"
 )
 
@@ -403,22 +404,13 @@
 
 }
 
-func containsBlock(blockID [32]byte, committedBlocks *CommittedBlockMap) bool {
-	for _, block := range *committedBlocks {
-		if bytes.Equal(block.ID(), blockID[:]) {
-			return true
-		}
-	}
-	return false
-}
-
-func GetBlockIDForView(view uint64, blockMap SafeBlockMap) [32]byte {
+func GetBlockIDForView(view uint64, blockMap SafeBlockMap) ([32]byte, error) {
 	for _, block := range blockMap {
 		if block.View == view {
-			return block.Hash()
+			return block.Hash(), nil
 		}
 	}
-	return nil
+	return [32]byte{}, fmt.Errorf("block not found for view %d", view)
 }
 
 func (node *Node) tryCommitGrandParent(block *Block, safeBlocks *SafeBlockMap, committedBlocks *CommittedBlockMap) {
@@ -435,7 +427,10 @@
 		parent.View == (grandParent.View + 1)
 	if canCommit {
 		for view := node.LatestCommittedView + 1; view <= grandParent.View; view++ {
-			blockHash := GetBlockIDForView(grandParent.View, *safeBlocks)
+			blockHash, err := GetBlockIDForView(view, *safeBlocks)
+			if err != nil {
+				return
+			}
 			block, ok := (*safeBlocks)[blockHash]
 			if !ok {
 				break
@@ -444,7 +439,7 @@
 				continue
 			}
 			(*committedBlocks)[block.Hash()] = block
-			incrementLatestCommittedView(view)
+			node.LatestCommittedView = view
 		}
 	}
 
@@ -564,7 +559,7 @@
 }
 
 // The handleBlockFromPeer is called whenever we receive a block from a peer.
-func handleBlockFromPeer(block *Block, node *Node) {
+func handleBlockFromPeer(block *Block, node *Node, safeblocks *SafeBlockMap, committedblocks *CommittedBlockMap) {
 	// Make sure that the block contains a valid QC, signature, transactions,
 	// and that it's for the current view.
 	if !sanityCheckBlock(*block, node) {
@@ -626,24 +621,7 @@
 	// Check if the chain looks like this:
 	// B1 - B2 - ... - B_current (current block)
 	// Where ... represent an arbitrary number of skipped views.
-	B_current := block
-	B2 := GetBlockForHash(block.QC.BlockHash)
-	B1 := GetBlockForHash(B2.QC.BlockHash)
-
-	// We update the highestQC if the block we received has one with a higher view.
-	// In the case where we have an AggregateQC, remember that B_current.QC will
-	// line up with the highest QC contained within the AggregateQC.
-	if B_current.QC.View > highestQC.View {
-		highestQC = B_current.QC
-	}
-
-	// Check that B2 is a direct child of B1.
-	if B2.View == B1.View+1 {
-		// A direct chain is formed between B1 and B2, reinforced by the QC
-		// in B_current. This means we should commit all blocks up to and including
-		// block B1.
-		CommitUpToBlock(B1)
-	}
+	node.tryCommitGrandParent(block, safeblocks, committedblocks)
 }
 
 func validateVote(vote VoteMessage) bool {
